## 2

https://www.kaggle.com/c/microsoft-malware-prediction/discussion/84065

> We did some basic label encoding, and then tried different algorithm. LightGBM, gave the best score, so we did parameter tuning with 10% of training data.

## 4

https://www.kaggle.com/c/microsoft-malware-prediction/discussion/84515

> I converted every categorical feature to a count encoding which included both train and test data in the counts. 
> I then averaged the predictions of LightGBM models trained over a 5 fold CV.

## 6

https://www.kaggle.com/c/microsoft-malware-prediction/discussion/84112

> Probing LB we were able to detect the switching date from Public to Private test sets in a date around 2018-10-26.
> We compared distribution of features in Public/Train and Private/Train and found many features with completely different distributions between Public and Private compared with train. It makes us think we must build models that generalizes well for Private and not for Public test set. 
> 
> Taking another look at the dates looks like train is sampled from 2 consecutive months from 2018-07-25 to 2018-09-25, Public from: 2018-09-26 to 2018-10-25 and Private from: 2018-10-26 to 2018-11-25. Knowing time series approaches there is no way to mimic Private set time behaviour using that 2 months train set only.
> 
> So in train we got first month of trainset to fit models and second month to validate.
> Using this splits and Target Encoding, I found many features that don't change much the performance from 1 month to another.
>
> The features that are most time dependents are: "AvSigVersion","EngineVersion", "AppVersion", "Census_OSBuildRevision", "Census_OSVersion", "OsVer", "OsBuild", "OsBuildLab", "OsPlatformSubRelease", "ProductName", "IsBeta", "DefaultBrowsersIdentifier", "IeVerIdentifier", "UacLuaenable", "Census_OSBranch", "Census_OSInstallTypeName", "HasTpm", "Platform", "Census_OSWUAutoUpdateOptionsName", "Census_FlightRing", "IsProtected", "Census_DeviceFamily", "Census_ProcessorClass", "Census_PrimaryDiskTypeName", "Census_IsPortableOperatingSystem", "Census_IsSecureBootEnabled", "Census_FlightRing", "Census_IsFlightsDisabled".
>
> On all my models I dropped the original versioning features ("AvSigVersion", "EngineVersion", "AppVersion", "Census_OSBuildRevision", "Census_OSVersion",), because most of the versions in test are not present in train and only used some minor feature engineering based on that features.
>
> I built several models, including some libFM, FFM, SVCs at the beggining and Keras DL and LighGBM. 
> But we found only Keras DL and LightGBM are aggregating value when blended, so our final solution used around 10x LGBs and 4x Keras DL models in a ranked geometric weighted average.
>
>  The trick of our solution is just to realized that Private testset is completely different from Public test and prepare the models taking it into account.

https://www.kaggle.com/c/microsoft-malware-prediction/discussion/84069

> In particular there are so many values appearing only in test for important features that ignoring the issue would be suicidal.
>
> Simple, using integer encoding of all version features, decomposing some into sub features, plus counting features as suggested by LongYin.
> 
> With embeddings, a value that only appear in test is a disaster as its embedding vector is just random.
> 
>  I therefore tried to find features that would not be time dependent but would still be applicable to original data. 
> After some exploration I found this plot, with AVSigVersion as y and EngineVersion as x, and color being the number of samples.
> It turns out that this feature alone increases private LB score by 0.02.
>
> We used LightGBM and Keras.

## 7

https://www.kaggle.com/c/microsoft-malware-prediction/discussion/86888

> I used LGBM with 40 features and used count encoding for most of the features.

## 9

https://www.kaggle.com/c/microsoft-malware-prediction/discussion/84570

> Firstly, we transformed some categorical columns which there are values with differences. E.g: in the feature SmartScreen exists “of”, “Off”, “off” and “OFF”. We transform these data to the same value: “OFF”.
> In NaN values, we transform with many criterials: the median, with value -1, and more.
>
> We transform with Label Encoding the categorical data, except in versions columns.
> In versions columnn we use two subset of these values. The first contain the first and second number, and the second contain the first, the second and the third number. E.g: the version “1.2.3.4” we transform to “1.2” and “1.2.3”.
>
> We use the dates shared in Discussions by Chris Deotte and add to dataset some values like differences between dates, STD, etc.
>
> We add eight groupings of columns. For example, we group the versions columns. For example, if there are 14 machines with same versions, we add the number 14 to data in this registers. Our intention with this strategy is remove time-dependence in the dataset. (with all data, train and test)
