```
>>> train_embedding.shape
(8921483, 81, 8)
>>> test_embedding.shape
(7853253, 81, 8)
>>> train.shape
(8921483, 7071)
>>> test.shape
(7853253, 7071)
```

长度都是一样的。 embedding 的形状是怎么回事呢？

```
self.emb_v2 = tf.get_variable(shape=[hparams.hash_ids, hparams.k], initializer=self.initializer, name='emb_v2')
emb_inp_v2 = tf.gather(self.emb_v2, self.features)
self.emb_inp_v2 = emb_inp_v2        
```

81 个 feature, 每个 feature 从 2e5 维的 hash_id 映射到了 8 维的 embedding。



```
>>> type(train)
<class 'scipy.sparse.csr.csr_matrix'>
>>> train.shape
(8921483, 7071)
>>> new_train.shape
(8921483, 648)
>>> type(new_train)
<class 'numpy.ndarray'>
```

如何 merge 不同种类的数据呢？



一直报 memory error 感觉是不大靠谱啊。

于是就写一个只用 embedding 的版本。