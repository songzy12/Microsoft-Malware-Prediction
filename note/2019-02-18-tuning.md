把 lgb 的 learning rate 从 0.05 改成 0.005 的效果：0.692 -> 0.693



使用 Lightgbm 的 Dart Mode 以及 Early Stopping 可能会有坑：

https://www.kaggle.com/c/microsoft-malware-prediction/discussion/78253



lgb 的参数列表：

https://sites.google.com/view/lauraepp/parameters

调参这个很有用：

https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html

> Following set of practices **can be used to improve your model efficiency.**
>
> 1. **num_leaves:** This is the main parameter to control the complexity of the tree model. Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting.
> 2. **min_data_in_leaf:** Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.
> 3. **max_depth:** You also can use max_depth to limit the tree depth explicitly.
>
> **For Faster Speed:**
>
> - Use bagging by setting `bagging_fraction` and `bagging_freq`
> - Use feature sub-sampling by setting `feature_fraction`
> - Use small `max_bin`
> - Use `save_binary` to speed up data loading in future learning
> - Use parallel learning, refer to [parallel learning guide](https://github.com/Microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.md).
>
> **For better accuracy:**
>
> - Use large `max_bin` (may be slower)
> - Use small `learning_rate` with large `num_iterations`
> - Use large `num_leaves`(may cause over-fitting)
> - Use bigger training data
> - Try `dart`
> - Try to use categorical feature directly
>
> **To deal with over-fitting:**
>
> - Use small `max_bin`
> - Use small `num_leaves`
> - Use `min_data_in_leaf` and `min_sum_hessian_in_leaf`
> - Use bagging by set `bagging_fraction` and `bagging_freq`
> - Use feature sub-sampling by set `feature_fraction`
> - Use bigger training data
> - Try `lambda_l1`, `lambda_l2` and `min_gain_to_split` to regularization
> - Try `max_depth` to avoid growing deep tree