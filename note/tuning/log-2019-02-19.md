这个的训练时间实在是太长了。感觉不值。

```
hparam = tf.contrib.training.HParams(
    model='xdeepfm',
    norm=True,
    batch_norm_decay=0.9,
    hidden_size=[128, 128],
    cross_layer_sizes=[128, 128, 128],
    k=8,
    hash_ids=int(2e5),
    batch_size=1024,
    optimizer="adam",
    learning_rate=0.001,
    num_display_steps=1000,
    num_eval_steps=1000,
    epoch=1,
    metric='auc',
    activation=['relu', 'relu', 'relu'],
    cross_activation='identity',
    init_method='uniform',
    init_value=0.1,
    feature_nums=len(features),
    kfold=5)
utils.print_hparams(hparam)
```

LB: 0.682. 并不如 LGBM 0.693，也不像 Kernel 里一样是  0.69.

<https://www.kaggle.com/guoday/xdeepfm-baseline>

和 LGBM 随便 blend 一下还是 0.693

所以这个训练结果还是有一定的随机性的。

```
Loading Train and Test Data.

  activation=['relu', 'relu', 'relu']
  batch_norm_decay=0.9
  batch_size=1024
  cross_activation=identity
  cross_layer_sizes=[128, 128, 128]
  epoch=1
  feature_nums=81
  hash_ids=200000
  hidden_size=[128, 128]
  init_method=uniform
  init_value=0.1
  k=8
  kfold=5                                                                       [118/354]
  learning_rate=0.001
  metric=auc
  model=xdeepfm
  norm=True
  num_display_steps=1000
  num_eval_steps=1000
  optimizer=adam
Number: 1748610
Number: 1748610
Number: 1748610
Number: 1748610
Number: 1748613
Fold 0
# Trainable variables
  emb_v1:0, (200000, 1),
  emb_v2:0, (200000, 8),
  Variable:0, (648, 128),
  norm_0/beta:0, (128,),
  norm_0/gamma:0, (128,),
  Variable_1:0, (128, 128),
  norm_1/beta:0, (128,),
  norm_1/gamma:0, (128,),
  Variable_2:0, (128, 1),
  exfm_part/f_0:0, (1, 6561, 128),
  exfm_part/f_1:0, (1, 5184, 128),
  exfm_part/f_2:0, (1, 5184, 128),
  exfm_part/w_nn_output:0, (256, 1),
  exfm_part/b_nn_output:0, (1,),
  epoch 0 step 1000 lr 0.001 logloss 0.623141 gN 0.29, Mon Feb 18 21:53:20 2019
# Epcho-time 1402.61s Eval AUC 0.719443. Best AUC 0.719443.
  epoch 0 step 2000 lr 0.001 logloss 0.606903 gN 0.23, Mon Feb 18 22:18:39 2019
# Epcho-time 2921.17s Eval AUC 0.729349. Best AUC 0.729349.
  epoch 0 step 3000 lr 0.001 logloss 0.602764 gN 0.21, Mon Feb 18 22:44:03 2019
# Epcho-time 4445.66s Eval AUC 0.732401. Best AUC 0.732401.
  epoch 0 step 4000 lr 0.001 logloss 0.599897 gN 0.20, Mon Feb 18 23:09:28 2019
# Epcho-time 5970.88s Eval AUC 0.734904. Best AUC 0.734904.
  epoch 0 step 5000 lr 0.001 logloss 0.598548 gN 0.20, Mon Feb 18 23:34:54 2019
# Epcho-time 7496.80s Eval AUC 0.736538. Best AUC 0.736538.
  epoch 0 step 6000 lr 0.001 logloss 0.597278 gN 0.19, Tue Feb 19 00:00:25 2019
# Epcho-time 9027.23s Eval AUC 0.737186. Best AUC 0.737186.
# Epcho-time 10311.53s Eval AUC 0.737831. Best AUC 0.737831.
# Epcho-time 10411.82s Eval AUC 0.737833. Best AUC 0.737833.
Training Done! Inference...

Fold 1
# Trainable variables
  emb_v1:0, (200000, 1),
  emb_v2:0, (200000, 8),
  Variable:0, (648, 128),
  norm_0/beta:0, (128,),
  norm_0/gamma:0, (128,),
  Variable_1:0, (128, 128),
  norm_1/beta:0, (128,),
  norm_1/gamma:0, (128,),
  Variable_2:0, (128, 1),
  exfm_part/f_0:0, (1, 6561, 128),
  exfm_part/f_1:0, (1, 5184, 128),
  exfm_part/f_2:0, (1, 5184, 128),
  exfm_part/w_nn_output:0, (256, 1),
  exfm_part/b_nn_output:0, (1,),
  epoch 0 step 1000 lr 0.001 logloss 0.621111 gN 0.29, Tue Feb 19 02:03:43 2019
# Epcho-time 1431.81s Eval AUC 0.724376. Best AUC 0.724376.
  epoch 0 step 2000 lr 0.001 logloss 0.606864 gN 0.23, Tue Feb 19 02:29:18 2019
# Epcho-time 2966.92s Eval AUC 0.731963. Best AUC 0.731963.
  epoch 0 step 3000 lr 0.001 logloss 0.602673 gN 0.21, Tue Feb 19 02:54:54 2019
# Epcho-time 4502.81s Eval AUC 0.734835. Best AUC 0.734835.
  epoch 0 step 4000 lr 0.001 logloss 0.599890 gN 0.20, Tue Feb 19 03:20:28 2019
# Epcho-time 6036.16s Eval AUC 0.737574. Best AUC 0.737574.
  epoch 0 step 5000 lr 0.001 logloss 0.598511 gN 0.19, Tue Feb 19 03:46:02 2019
# Epcho-time 7570.75s Eval AUC 0.738567. Best AUC 0.738567.
  epoch 0 step 6000 lr 0.001 logloss 0.597253 gN 0.19, Tue Feb 19 04:11:40 2019
# Epcho-time 9108.56s Eval AUC 0.739841. Best AUC 0.739841.
# Epcho-time 10403.32s Eval AUC 0.740136. Best AUC 0.740136.
# Epcho-time 10503.85s Eval AUC 0.740139. Best AUC 0.740139.
Training Done! Inference...
Fold 2
# Trainable variables
  emb_v1:0, (200000, 1),
  emb_v2:0, (200000, 8),
  Variable:0, (648, 128),
  norm_0/beta:0, (128,),
  norm_0/gamma:0, (128,),
  Variable_1:0, (128, 128),
  norm_1/beta:0, (128,),
  norm_1/gamma:0, (128,),
  Variable_2:0, (128, 1),
  exfm_part/f_0:0, (1, 6561, 128),
  exfm_part/f_1:0, (1, 5184, 128),
  exfm_part/f_2:0, (1, 5184, 128),
  exfm_part/w_nn_output:0, (256, 1),
  exfm_part/b_nn_output:0, (1,),
  epoch 0 step 1000 lr 0.001 logloss 0.622461 gN 0.31, Tue Feb 19 06:15:22 2019
# Epcho-time 1436.98s Eval AUC 0.722621. Best AUC 0.722621.
  epoch 0 step 2000 lr 0.001 logloss 0.607036 gN 0.24, Tue Feb 19 06:40:59 2019
# Epcho-time 2974.53s Eval AUC 0.729968. Best AUC 0.729968.
  epoch 0 step 3000 lr 0.001 logloss 0.602678 gN 0.22, Tue Feb 19 07:06:38 2019
# Epcho-time 4512.97s Eval AUC 0.733098. Best AUC 0.733098.
  epoch 0 step 4000 lr 0.001 logloss 0.599393 gN 0.20, Tue Feb 19 07:32:17 2019
# Epcho-time 6052.64s Eval AUC 0.736397. Best AUC 0.736397.
  epoch 0 step 5000 lr 0.001 logloss 0.598287 gN 0.20, Tue Feb 19 07:57:57 2019
# Epcho-time 7591.84s Eval AUC 0.737298. Best AUC 0.737298.
  epoch 0 step 6000 lr 0.001 logloss 0.597196 gN 0.19, Tue Feb 19 08:23:34 2019
# Epcho-time 9129.62s Eval AUC 0.738583. Best AUC 0.738583.
# Epcho-time 10425.17s Eval AUC 0.738824. Best AUC 0.738824.
# Epcho-time 10525.67s Eval AUC 0.738824. Best AUC 0.738824.
Training Done! Inference...
Fold 3
# Trainable variables
  emb_v1:0, (200000, 1),
  emb_v2:0, (200000, 8),
  Variable:0, (648, 128),
  norm_0/beta:0, (128,),
  norm_0/gamma:0, (128,),
  Variable_1:0, (128, 128),
  norm_1/beta:0, (128,),
  norm_1/gamma:0, (128,),
  Variable_2:0, (128, 1),
  exfm_part/f_0:0, (1, 6561, 128),
  exfm_part/f_1:0, (1, 5184, 128),
  exfm_part/f_2:0, (1, 5184, 128),
  exfm_part/w_nn_output:0, (256, 1),
  exfm_part/b_nn_output:0, (1,),
  epoch 0 step 1000 lr 0.001 logloss 0.621951 gN 0.31, Tue Feb 19 10:28:15 2019
# Epcho-time 1444.56s Eval AUC 0.720170. Best AUC 0.720170.
  epoch 0 step 2000 lr 0.001 logloss 0.606956 gN 0.23, Tue Feb 19 10:53:58 2019
# Epcho-time 2987.21s Eval AUC 0.727365. Best AUC 0.727365.
  epoch 0 step 3000 lr 0.001 logloss 0.602630 gN 0.21, Tue Feb 19 11:19:45 2019
# Epcho-time 4534.52s Eval AUC 0.730910. Best AUC 0.730910.
  epoch 0 step 4000 lr 0.001 logloss 0.599880 gN 0.20, Tue Feb 19 11:45:30 2019
# Epcho-time 6079.07s Eval AUC 0.733815. Best AUC 0.733815.
  epoch 0 step 5000 lr 0.001 logloss 0.598796 gN 0.19, Tue Feb 19 12:11:13 2019
# Epcho-time 7622.43s Eval AUC 0.734899. Best AUC 0.734899.
  epoch 0 step 6000 lr 0.001 logloss 0.597151 gN 0.19, Tue Feb 19 12:37:02 2019
# Epcho-time 9171.26s Eval AUC 0.736826. Best AUC 0.736826.
# Epcho-time 10477.70s Eval AUC 0.736637. Best AUC 0.736826.
# Epcho-time 10577.33s Eval AUC 0.736826. Best AUC 0.736826.
Training Done! Inference...
Fold 4
# Trainable variables
  emb_v1:0, (200000, 1),
  emb_v2:0, (200000, 8),
  Variable:0, (648, 128),
  norm_0/beta:0, (128,),
  norm_0/gamma:0, (128,),
  Variable_1:0, (128, 128),
  norm_1/beta:0, (128,),
  norm_1/gamma:0, (128,),
  Variable_2:0, (128, 1),
  exfm_part/f_0:0, (1, 6561, 128),
  exfm_part/f_1:0, (1, 5184, 128),
  exfm_part/f_2:0, (1, 5184, 128),
  exfm_part/w_nn_output:0, (256, 1),
  exfm_part/b_nn_output:0, (1,),
  epoch 0 step 1000 lr 0.001 logloss 0.620872 gN 0.29, Tue Feb 19 14:41:17 2019
# Epcho-time 1449.23s Eval AUC 0.721362. Best AUC 0.721362.
  epoch 0 step 2000 lr 0.001 logloss 0.606439 gN 0.23, Tue Feb 19 15:07:02 2019
# Epcho-time 2993.96s Eval AUC 0.728409. Best AUC 0.728409.
  epoch 0 step 3000 lr 0.001 logloss 0.602628 gN 0.21, Tue Feb 19 15:32:50 2019
# Epcho-time 4542.19s Eval AUC 0.732168. Best AUC 0.732168.
  epoch 0 step 4000 lr 0.001 logloss 0.599871 gN 0.20, Tue Feb 19 15:58:43 2019
# Epcho-time 6094.98s Eval AUC 0.734709. Best AUC 0.734709.
  epoch 0 step 5000 lr 0.001 logloss 0.598684 gN 0.19, Tue Feb 19 16:24:26 2019
# Epcho-time 7637.73s Eval AUC 0.736182. Best AUC 0.736182.
  epoch 0 step 6000 lr 0.001 logloss 0.596889 gN 0.19, Tue Feb 19 16:50:07 2019
# Epcho-time 9179.17s Eval AUC 0.737152. Best AUC 0.737152.
# Epcho-time 10473.57s Eval AUC 0.738188. Best AUC 0.738188.
# Epcho-time 10574.67s Eval AUC 0.738188. Best AUC 0.738188.
Training Done! Inference...
0    0.513848
1    0.428136
2    0.415677
3    0.352981
4    0.451118
Name: HasDetections, dtype: float32
```