LB: 0.689

blend with lgb (0.693): 0.696

```
Loading Train and Test Data.

  activation=['relu', 'relu', 'relu']
  batch_norm_decay=0.9
  batch_size=1024
  cross_activation=identity
  cross_layer_sizes=[1024, 512, 128]
  epoch=1
  feature_nums=81
  hash_ids=200000
  hidden_size=[1024, 128]
  init_method=uniform
  init_value=0.1
  k=8
  kfold=5
  learning_rate=0.001
  metric=auc
  model=nffm
  norm=True
  num_display_steps=1000
  num_eval_steps=1000
  optimizer=adam
Number: 1748610
Number: 1748610
Number: 1748610
Number: 1748610
Number: 1748613
Fold 0
# Trainable variables
  emb_v1:0, (200000, 1), 
  emb_v2:0, (200000, 81, 8), 
  Variable:0, (3240, 1024), 
  norm_0/beta:0, (1024,), 
  norm_0/gamma:0, (1024,), 
  Variable_1:0, (1024, 128), 
  norm_1/beta:0, (128,), 
  norm_1/gamma:0, (128,), 
  Variable_2:0, (128, 1), 
  Variable_3:0, (), 
  epoch 0 step 1000 lr 0.001 logloss 0.626965 gN 0.22, Wed Feb 20 15:46:30 2019
# Epcho-time 1291.14s Eval AUC 0.725440. Best AUC 0.725440.
  epoch 0 step 2000 lr 0.001 logloss 0.602682 gN 0.16, Wed Feb 20 16:09:19 2019
# Epcho-time 2659.82s Eval AUC 0.732798. Best AUC 0.732798.
  epoch 0 step 3000 lr 0.001 logloss 0.598869 gN 0.14, Wed Feb 20 16:32:15 2019
# Epcho-time 4035.64s Eval AUC 0.734863. Best AUC 0.734863.
  epoch 0 step 4000 lr 0.001 logloss 0.597289 gN 0.14, Wed Feb 20 16:55:05 2019
# Epcho-time 5406.50s Eval AUC 0.737132. Best AUC 0.737132.
  epoch 0 step 5000 lr 0.001 logloss 0.594901 gN 0.13, Wed Feb 20 17:18:08 2019
# Epcho-time 6789.07s Eval AUC 0.739537. Best AUC 0.739537.
  epoch 0 step 6000 lr 0.001 logloss 0.593154 gN 0.13, Wed Feb 20 17:41:05 2019
# Epcho-time 8165.77s Eval AUC 0.741024. Best AUC 0.741024.
# Epcho-time 9320.71s Eval AUC 0.742088. Best AUC 0.742088.
# Epcho-time 9388.96s Eval AUC 0.742122. Best AUC 0.742122.
Training Done! Inference...
Fold 1
# Trainable variables
  emb_v1:0, (200000, 1), 
  emb_v2:0, (200000, 81, 8), 
  Variable:0, (3240, 1024), 
  norm_0/beta:0, (1024,), 
  norm_0/gamma:0, (1024,), 
  Variable_1:0, (1024, 128), 
  norm_1/beta:0, (128,), 
  norm_1/gamma:0, (128,), 
  Variable_2:0, (128, 1), 
  Variable_3:0, (), 
  epoch 0 step 1000 lr 0.001 logloss 0.628923 gN 0.22, Wed Feb 20 19:13:53 2019
# Epcho-time 1309.97s Eval AUC 0.724678. Best AUC 0.724678.
  epoch 0 step 2000 lr 0.001 logloss 0.602554 gN 0.15, Wed Feb 20 19:36:48 2019
# Epcho-time 2685.15s Eval AUC 0.732282. Best AUC 0.732282.
  epoch 0 step 3000 lr 0.001 logloss 0.598930 gN 0.14, Wed Feb 20 19:59:48 2019
# Epcho-time 4065.52s Eval AUC 0.735269. Best AUC 0.735269.
  epoch 0 step 4000 lr 0.001 logloss 0.597136 gN 0.14, Wed Feb 20 20:22:47 2019
# Epcho-time 5444.14s Eval AUC 0.736121. Best AUC 0.736121.
  epoch 0 step 5000 lr 0.001 logloss 0.594783 gN 0.13, Wed Feb 20 20:45:42 2019
# Epcho-time 6819.82s Eval AUC 0.739573. Best AUC 0.739573.
  epoch 0 step 6000 lr 0.001 logloss 0.593186 gN 0.13, Wed Feb 20 21:08:45 2019
# Epcho-time 8201.94s Eval AUC 0.740183. Best AUC 0.740183.
# Epcho-time 9382.59s Eval AUC 0.741856. Best AUC 0.741856.
# Epcho-time 9450.62s Eval AUC 0.741869. Best AUC 0.741869.
Training Done! Inference...
Fold 2
# Trainable variables
  emb_v1:0, (200000, 1), 
  emb_v2:0, (200000, 81, 8), 
  Variable:0, (3240, 1024), 
  norm_0/beta:0, (1024,), 
  norm_0/gamma:0, (1024,), 
  Variable_1:0, (1024, 128), 
  norm_1/beta:0, (128,), 
  norm_1/gamma:0, (128,), 
  Variable_2:0, (128, 1), 
  Variable_3:0, (), 
  epoch 0 step 1000 lr 0.001 logloss 0.626098 gN 0.22, Wed Feb 20 22:42:38 2019
# Epcho-time 1340.86s Eval AUC 0.726525. Best AUC 0.726525.
  epoch 0 step 2000 lr 0.001 logloss 0.603053 gN 0.16, Wed Feb 20 23:05:54 2019
# Epcho-time 2736.38s Eval AUC 0.733283. Best AUC 0.733283.
  epoch 0 step 3000 lr 0.001 logloss 0.599017 gN 0.15, Wed Feb 20 23:29:28 2019
# Epcho-time 4150.57s Eval AUC 0.734576. Best AUC 0.734576.
  epoch 0 step 4000 lr 0.001 logloss 0.596822 gN 0.15, Wed Feb 20 23:53:29 2019
# Epcho-time 5591.94s Eval AUC 0.738383. Best AUC 0.738383.
  epoch 0 step 5000 lr 0.001 logloss 0.594860 gN 0.14, Thu Feb 21 00:17:02 2019
# Epcho-time 7004.49s Eval AUC 0.740810. Best AUC 0.740810.
  epoch 0 step 6000 lr 0.001 logloss 0.593308 gN 0.14, Thu Feb 21 00:40:25 2019
# Epcho-time 8407.47s Eval AUC 0.742610. Best AUC 0.742610.
# Epcho-time 9593.72s Eval AUC 0.743864. Best AUC 0.743864.
# Epcho-time 9662.57s Eval AUC 0.743895. Best AUC 0.743895.
Training Done! Inference...
Fold 3
# Trainable variables
  emb_v1:0, (200000, 1), 
  emb_v2:0, (200000, 81, 8), 
  Variable:0, (3240, 1024), 
  norm_0/beta:0, (1024,), 
  norm_0/gamma:0, (1024,), 
  Variable_1:0, (1024, 128), 
  norm_1/beta:0, (128,), 
  norm_1/gamma:0, (128,), 
  Variable_2:0, (128, 1), 
  Variable_3:0, (), 
  epoch 0 step 1000 lr 0.001 logloss 0.627247 gN 0.22, Thu Feb 21 02:14:52 2019
# Epcho-time 1335.60s Eval AUC 0.727036. Best AUC 0.727036.
  epoch 0 step 2000 lr 0.001 logloss 0.602944 gN 0.16, Thu Feb 21 02:38:21 2019
# Epcho-time 2744.67s Eval AUC 0.735184. Best AUC 0.735184.
  epoch 0 step 3000 lr 0.001 logloss 0.599122 gN 0.15, Thu Feb 21 03:01:40 2019
# Epcho-time 4143.84s Eval AUC 0.734715. Best AUC 0.735184.
  epoch 0 step 4000 lr 0.001 logloss 0.596054 gN 0.14, Thu Feb 21 03:25:05 2019
# Epcho-time 5548.53s Eval AUC 0.740428. Best AUC 0.740428.
  epoch 0 step 5000 lr 0.001 logloss 0.595351 gN 0.13, Thu Feb 21 03:48:39 2019
# Epcho-time 6962.56s Eval AUC 0.740771. Best AUC 0.740771.
  epoch 0 step 6000 lr 0.001 logloss 0.593499 gN 0.13, Thu Feb 21 04:12:15 2019
# Epcho-time 8378.70s Eval AUC 0.742789. Best AUC 0.742789.
# Epcho-time 9564.38s Eval AUC 0.743875. Best AUC 0.743875.
# Epcho-time 9633.03s Eval AUC 0.743871. Best AUC 0.743875.
Training Done! Inference...
Fold 4
# Trainable variables
  emb_v1:0, (200000, 1), 
  emb_v2:0, (200000, 81, 8), 
  Variable:0, (3240, 1024), 
  norm_0/beta:0, (1024,), 
  norm_0/gamma:0, (1024,), 
  Variable_1:0, (1024, 128), 
  norm_1/beta:0, (128,), 
  norm_1/gamma:0, (128,), 
  Variable_2:0, (128, 1), 
  Variable_3:0, (), 
  epoch 0 step 1000 lr 0.001 logloss 0.640050 gN 0.25, Thu Feb 21 05:47:00 2019
# Epcho-time 1338.41s Eval AUC 0.727970. Best AUC 0.727970.
  epoch 0 step 2000 lr 0.001 logloss 0.603346 gN 0.16, Thu Feb 21 06:10:29 2019
# Epcho-time 2746.86s Eval AUC 0.734807. Best AUC 0.734807.
  epoch 0 step 3000 lr 0.001 logloss 0.599341 gN 0.16, Thu Feb 21 06:33:57 2019
# Epcho-time 4155.04s Eval AUC 0.735792. Best AUC 0.735792.
  epoch 0 step 4000 lr 0.001 logloss 0.596387 gN 0.15, Thu Feb 21 06:57:30 2019
# Epcho-time 5568.51s Eval AUC 0.740956. Best AUC 0.740956.
  epoch 0 step 5000 lr 0.001 logloss 0.595625 gN 0.14, Thu Feb 21 07:20:57 2019
# Epcho-time 6974.88s Eval AUC 0.741834. Best AUC 0.741834.
  epoch 0 step 6000 lr 0.001 logloss 0.594575 gN 0.13, Thu Feb 21 07:44:28 2019
# Epcho-time 8385.81s Eval AUC 0.743117. Best AUC 0.743117.
# Epcho-time 9563.53s Eval AUC 0.744256. Best AUC 0.744256.
# Epcho-time 9632.96s Eval AUC 0.744273. Best AUC 0.744273.
/home/songzy/.virtualenvs/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Training Done! Inference...
0    0.515432
1    0.577233
2    0.474734
3    0.339281
4    0.415856
Name: HasDetections, dtype: float32
```

